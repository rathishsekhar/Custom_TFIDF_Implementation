{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9464I-uxLiw"
   },
   "source": [
    "# Custom TF-IDF implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we endeavor to replicate the TFIDF vectorizer functionality found in Scikit-learn's sklearn library. This replication process is divided into two distinct tasks, namely Task 1 and Task 2.\n",
    "\n",
    "In Task 1, our objective is to faithfully reproduce the behavior of the fit and transform functions provided by Scikit-learn. Subsequently, we will compare the outcomes of our implementation with those obtained using Scikit-learn's functionality.\n",
    "\n",
    "Moving on to Task 2, our focus shifts towards extracting the top 50 features. Once again, we will meticulously compare our results to those generated by Scikit-learn's library.\n",
    "\n",
    "For more comprehensive details and precise definitions, please consult the readme document located in the current folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg2ooa4DxLiz"
   },
   "source": [
    "## Task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "# Collection of string documents - generating a collection of string documents\n",
    "# Taken from sklearn.feature_extraction.text.TfidfVectorizer exampls\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "### SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Np4dfQOkxLi4"
   },
   "outputs": [],
   "source": [
    "# Importing TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-7Om8YpYxLi6",
    "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dTKplK96xLi-",
    "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-CTiWHygxLjA",
    "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bDKEpbA-xLjD",
    "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3QWo34hexLjF",
    "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# We are converting the sparse output matrix to dense matrix and printing it.\n",
    "# The output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "### Custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "HjuCcJwXxLjJ"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "# Adding imports for fit and transform functions\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a set of functions that will cover the task 1: \n",
    "# Fit function based on reference document\n",
    "def fit(dataset):\n",
    "    '''\n",
    "    Inputs a list of text sentences and outputs \n",
    "    the vocabulary dictionary in alphabetical order\n",
    "    '''    \n",
    "    uniquewords = set()\n",
    "    # Ensuring that the dataset entered is in the list format\n",
    "    if isinstance(dataset, list):\n",
    "        for row in dataset: # for each review in the dataset\n",
    "            for word in row.split(\" \"): # for each word in the review. Split method converts a string into list of words\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                uniquewords.add(word)\n",
    "        uniquewords = sorted(list(uniquewords))\n",
    "        vocab = {j:i for i,j in enumerate(uniquewords)}\n",
    "        \n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"Enter the correct format of the data in list\")\n",
    "        \n",
    "# Transform function based on the reference document\n",
    "def transform(dataset, vocab):\n",
    "    '''\n",
    "    Inputs the dataset and Vocabulary dictionary, both inputed by the user. \n",
    "    The vocab inputted is the output of the fit function. \n",
    "    This function generates a sparse martrix comprising of the counts of a word \n",
    "    occurence for all documents in the corpus. this function is rewritten from the reference document \n",
    "    provided\n",
    "    '''\n",
    "    \n",
    "    rowval, colval, valval = [], [], []\n",
    "    \n",
    "    # Ensuring the dataset is in the list format\n",
    "    if isinstance(dataset, list):\n",
    "        for x, y in enumerate(dataset): # y = documnet, x = index\n",
    "            doc_word_dict = dict(Counter(y.split(\" \"))) # Generating dict containing each words as key and number of times of occurence in values\n",
    "            for a, b in doc_word_dict.items(): # a = word in a doc, b = number of times the word has occured\n",
    "                if len(a) < 2: # Not considering single words (to remove punctuations and words like a, I etc...)\n",
    "                    continue # Ensuring the outer for loop continues if the word has < 2 \n",
    "                # Checking word with vocab dict    \n",
    "                m = vocab.get(a, -1) # Returns value -1 if the word isn't available\n",
    "                if m != -1:\n",
    "                    rowval.append(x) # Taking the value of the index number of the doc\n",
    "                    colval.append(m) # Taking the value of the word's column number from vocab\n",
    "                    valval.append(b) # Taking the value of the frequency from the Counter function for a given word in the doc\n",
    "        \n",
    "        return csr_matrix((valval, (rowval,colval)), shape = (len(dataset), len(vocab)))\n",
    "    \n",
    "    else: \n",
    "        return \"The given dataset doesn't contain a list of sentances\"\n",
    "\n",
    "def termfreq(matrix):\n",
    "    '''\n",
    "    Provides a matrix where each element is \n",
    "    the term frequency value of the of inputed \n",
    "    matrix\n",
    "    ''' \n",
    "    return matrix/matrix.sum(axis = 1)\n",
    "\n",
    "def idf_(matrix):\n",
    "    '''\n",
    "    Provides a matrix where each element is the Inverse \n",
    "    document frequency of the inputted matrix\n",
    "    The Scikit learn formulae for IDF can be interpretted as: \n",
    "    1+logð‘’(1 + Total number of documents in collection1) - log(1+Number of documents with term t in it)\n",
    "    = 1+ log1p(Total number of documents in collection) - logp(1+ number of document with term t in it)\n",
    "    = 1 + log(1 + total number of document/ number of rows) - log1p(number of document with term t in it)\n",
    "    = 1 + log(1 + total number of document/ number of rows) - log1p(COLUMN non-zero sum)\n",
    "    '''\n",
    "    # get_shape for scipy.csr_matrix.get_shape provides the shape of the csr matrix, we are trying to get the rowcount or doc count\n",
    "    # Calculating the log of (1 + total number of documents)\n",
    "    \n",
    "    return 1 + math.log(1 + matrix.get_shape()[0]) - np.log1p(matrix.getnnz(axis = 0))\n",
    "\n",
    "def tfidfvectorzr(tf, idf):\n",
    "    '''\n",
    "    multiplies the corresponding idf value with the\n",
    "    tf value and returns a sparse matrix\n",
    "    '''\n",
    "    return csr_matrix(normalize(np.multiply(tf.toarray(),idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names generated by user generated fit function:\n",
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n",
      "--------------------------------------------------\n",
      "Feature name generated by SkLearn:\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "==================================================\n",
      "\n",
      "User's idf_ function generated idf values\n",
      "\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n",
      "--------------------------------------------------\n",
      "\n",
      "SkLearn's Vectorizer generated idf values:\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n",
      "==================================================\n",
      "\n",
      "TFIDF final vector value calculated\n",
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 3)\t0.38408524091481494\n",
      "  (0, 6)\t0.38408524091481494\n",
      "  (0, 8)\t0.38408524091481494\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 5)\t0.5386476208856762\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.4697913855799205\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 3)\t0.38408524091481494\n",
      "  (3, 6)\t0.38408524091481494\n",
      "  (3, 8)\t0.38408524091481494\n",
      "--------------------------------------------------\n",
      "\n",
      "TFIDF as calculated from the SciKit learn package\n",
      "  (0, 1)\t0.46979138557992045\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 8)\t0.38408524091481483\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.46979138557992045\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 8)\t0.38408524091481483\n"
     ]
    }
   ],
   "source": [
    "# Testing the data with corpus below and comapring with the SkLearn's implementation:\n",
    "\n",
    "corpus = ['this is the first document',\n",
    "      'this document is the second document',\n",
    "      'and this is the third one',\n",
    "      'is this the first document']\n",
    "\n",
    "#SkLearn's code for corpus (copied from assignment above)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorz = TfidfVectorizer()\n",
    "vectorz.fit(corpus)\n",
    "skl_output = vectorz.transform(corpus)\n",
    "\n",
    "# checking if the vocabulary from fit function matches with SkLearn's get_featurenames()\n",
    "vocabulary = fit(corpus)\n",
    "print(\"Feature names generated by user generated fit function:\")\n",
    "print(vocabulary)\n",
    "print(\"-\"*50)\n",
    "print(\"Feature name generated by SkLearn:\")\n",
    "print(vectorz.get_feature_names_out())\n",
    "\n",
    "# checking if idf values derived from  matches with SkLearn's output:\n",
    "print(\"=\"*50)\n",
    "y = transform(corpus, vocabulary)\n",
    "w = termfreq(y)\n",
    "z = idf_(y)\n",
    "\n",
    "print(\"\\nUser\\'s idf_ function generated idf values\")\n",
    "print()\n",
    "print(z)\n",
    "print(\"-\"*50)\n",
    "print(\"\\nSkLearn\\'s Vectorizer generated idf values:\")\n",
    "print(vectorz.idf_)\n",
    "\n",
    "# Testing the tfidfvectorizer formulae:\n",
    "# Calculation of final TFIDF\n",
    "print(\"=\"*50)\n",
    "print(\"\\nTFIDF final vector value calculated\")\n",
    "print(tfidfvectorzr(w,z))\n",
    "\n",
    "#Calculation of final TFIDF from SciKit learn\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTFIDF as calculated from the SciKit learn package\")\n",
    "print(vectorz.fit_transform(corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result for Task 1:\n",
    "\n",
    "The TFIDF vectors generated by both Scikit-learn and our custom code have produced exactly the same values, confirming the success of our custom implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMxBmVZExLjK"
   },
   "source": [
    "## Task-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rathish/Documents/Data_Science_Projects/Custom_TFIDF_Implementation/data\n"
     ]
    }
   ],
   "source": [
    "#local directories:\n",
    "local = '/Users/rathish/Documents/Data_Science_Projects/Custom_TFIDF_Implementation/data'\n",
    "%cd $local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a set of modified functions taken from task 1 to complete task 2: \n",
    "# The fit, transform and termfreq functions do not change \n",
    "\n",
    "# Creating a tf-idf function which generates tf and idf valued matrix using the previously defined fit, transpform and termfreq:\n",
    "def tf_idf(corpus, top_n_featnames):\n",
    "    '''\n",
    "    This funtion generates both tf and idf directly from the input of corpus and top_n_feat names. The dictionary with topfeatures is returned as a third output\n",
    "     This funtion uses fit, transform, termfreq funcion and idf function\n",
    "     '''\n",
    "    # Repeating the first four steps conducted in task 1 to obtain a full idf for the data (so far top_n_featnames haven't been considered)\n",
    "    vocab = fit(corpus)\n",
    "    transform_full = transform(corpus, vocab)\n",
    "    tf = termfreq(transform_full)\n",
    "    fullidfvector = idf_(tf)\n",
    "    # Associating the calculated idf values to vocab generated by fit function, output: dict(idf:word)\n",
    "    fullidfdict = dict([(k,v) for k,v in zip(list(vocab.keys()),fullidfvector.tolist())])\n",
    "    # Reordering and creating vocab with top 50 features. All other feature values to be marked to zero\n",
    "    sortedtuplist = sorted(fullidfdict.items(), key=lambda x: x[1], reverse = True)\n",
    "    \n",
    "    # Creating two lists to append vocab words and idf values\n",
    "    m = []\n",
    "    n = []\n",
    "    for x in sortedtuplist:\n",
    "        m.append(x[0])\n",
    "        n.append(x[1])\n",
    "    # Making the idf values for vocab which are not in the top_n_featnames zero\n",
    "    for y in range(top_n_featnames, len(m)):\n",
    "        n[y] = 0\n",
    "    \n",
    "    topfeatdict = dict(zip(m,n))\n",
    "    \n",
    "    # Creating another dict that contains only the top_n_features\n",
    "    rettopfeatdict = topfeatdict.copy()\n",
    "    for feat in topfeatdict.keys():\n",
    "        if topfeatdict[feat] == 0:\n",
    "            del rettopfeatdict[feat]\n",
    "   \n",
    "    collist, idflist = [], []\n",
    "    # Reconstructing new TF and IDF matrices \n",
    "    for feat in sorted(topfeatdict.keys()):\n",
    "        if topfeatdict[feat] != 0:\n",
    "            colnum = vocab[feat]\n",
    "            csrmat = tf.toarray()[:,colnum]\n",
    "            collist.append(csrmat.reshape(-1,1)) # Getting col using the col num and appending it to collist(list)\n",
    "            \n",
    "    tftopfeat = np.hstack(collist) # Regular matrix with top feature names\n",
    "    \n",
    "    # Generating a row matrix with top idf values\n",
    "    \n",
    "    for idf in sorted(topfeatdict.keys()):\n",
    "        if topfeatdict[idf] != 0:\n",
    "            idflist.append(topfeatdict[idf])\n",
    "    \n",
    "    idf_topfeat = np.array(idflist)\n",
    "   \n",
    "    \n",
    "    return tftopfeat, idf_topfeat, rettopfeatdict\n",
    "    \n",
    "    \n",
    "def tfidfvectorzr2(tf, idf):\n",
    "    '''\n",
    "    multiplies the corresponding idf value with the\n",
    "    tf value and returns a sparse matrix\n",
    "    ''' \n",
    "    # Generating a TF - CSR matrix with only top feature names\n",
    "    return csr_matrix(normalize(np.multiply(tf,idf)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of custom TFIDF vectorizer = (746, 50) \n",
      "\n",
      " The shape of sklearn TFIDF Vectorizer =\n",
      "{'aailiyah': 6.922918004572872, 'abandoned': 6.922918004572872, 'abroad': 6.922918004572872, 'abstruse': 6.922918004572872, 'academy': 6.922918004572872, 'accents': 6.922918004572872, 'accessible': 6.922918004572872, 'acclaimed': 6.922918004572872, 'accolades': 6.922918004572872, 'accurate': 6.922918004572872, 'accurately': 6.922918004572872, 'achille': 6.922918004572872, 'ackerman': 6.922918004572872, 'actions': 6.922918004572872, 'adams': 6.922918004572872, 'add': 6.922918004572872, 'added': 6.922918004572872, 'admins': 6.922918004572872, 'admiration': 6.922918004572872, 'admitted': 6.922918004572872, 'adrift': 6.922918004572872, 'adventure': 6.922918004572872, 'aesthetically': 6.922918004572872, 'affected': 6.922918004572872, 'affleck': 6.922918004572872, 'afternoon': 6.922918004572872, 'aged': 6.922918004572872, 'ages': 6.922918004572872, 'agree': 6.922918004572872, 'agreed': 6.922918004572872, 'aimless': 6.922918004572872, 'aired': 6.922918004572872, 'akasha': 6.922918004572872, 'akin': 6.922918004572872, 'alert': 6.922918004572872, 'alike': 6.922918004572872, 'allison': 6.922918004572872, 'allow': 6.922918004572872, 'allowing': 6.922918004572872, 'alongside': 6.922918004572872, 'amateurish': 6.922918004572872, 'amaze': 6.922918004572872, 'amazed': 6.922918004572872, 'amazingly': 6.922918004572872, 'amusing': 6.922918004572872, 'amust': 6.922918004572872, 'anatomist': 6.922918004572872, 'angel': 6.922918004572872, 'angela': 6.922918004572872, 'angelina': 6.922918004572872} \n",
      " ['acting' 'actors' 'also' 'bad' 'best' 'better' 'cast' 'character'\n",
      " 'characters' 'could' 'even' 'ever' 'every' 'excellent' 'film' 'films'\n",
      " 'funny' 'good' 'great' 'like' 'little' 'look' 'love' 'made' 'make'\n",
      " 'movie' 'movies' 'much' 'never' 'no' 'not' 'one' 'plot' 'real' 'really'\n",
      " 'scenes' 'script' 'see' 'seen' 'show' 'story' 'think' 'time' 'watch'\n",
      " 'watching' 'way' 'well' 'wonderful' 'work' 'would']\n",
      "================================================== \n",
      "Custom TFIDF vector value\n",
      "  (0, 30)\t1.0\n",
      "  (68, 24)\t1.0\n",
      "  (72, 29)\t1.0\n",
      "  (74, 31)\t1.0\n",
      "  (119, 33)\t1.0\n",
      "  (135, 3)\t0.3779644730092272\n",
      "  (135, 10)\t0.3779644730092272\n",
      "  (135, 18)\t0.3779644730092272\n",
      "  (135, 20)\t0.3779644730092272\n",
      "  (135, 36)\t0.3779644730092272\n",
      "  (135, 40)\t0.3779644730092272\n",
      "  (135, 41)\t0.3779644730092272\n",
      "  (176, 49)\t1.0\n",
      "  (181, 13)\t1.0\n",
      "  (192, 21)\t1.0\n",
      "  (193, 23)\t1.0\n",
      "  (216, 2)\t1.0\n",
      "  (222, 47)\t1.0\n",
      "  (225, 19)\t1.0\n",
      "  (227, 17)\t1.0\n",
      "  (241, 44)\t1.0\n",
      "  (270, 1)\t1.0\n",
      "  (290, 25)\t1.0\n",
      "  (333, 26)\t1.0\n",
      "  (334, 15)\t1.0\n",
      "  (341, 43)\t1.0\n",
      "  (344, 42)\t1.0\n",
      "  (348, 8)\t1.0\n",
      "  (377, 37)\t1.0\n",
      "  (409, 5)\t1.0\n",
      "  (430, 39)\t1.0\n",
      "  (457, 45)\t1.0\n",
      "  (461, 4)\t1.0\n",
      "  (465, 38)\t1.0\n",
      "  (475, 35)\t1.0\n",
      "  (493, 6)\t1.0\n",
      "  (500, 48)\t1.0\n",
      "  (548, 0)\t0.7071067811865475\n",
      "  (548, 32)\t0.7071067811865475\n",
      "  (608, 14)\t1.0\n",
      "  (612, 11)\t1.0\n",
      "  (620, 46)\t1.0\n",
      "  (632, 7)\t1.0\n",
      "  (644, 12)\t0.7071067811865476\n",
      "  (644, 27)\t0.7071067811865476\n",
      "  (664, 28)\t1.0\n",
      "  (667, 22)\t1.0\n",
      "  (691, 34)\t1.0\n",
      "  (697, 9)\t1.0\n",
      "  (722, 16)\t1.0\n",
      "\n",
      "\n",
      " Sklearn TFIDF vector value: \n",
      "  (0, 25)\t1.0\n",
      "  (1, 8)\t0.8199426324888012\n",
      "  (1, 30)\t0.572445699981522\n",
      "  (2, 32)\t0.5699230219852334\n",
      "  (2, 0)\t0.5202944244602254\n",
      "  (2, 10)\t0.5273651853196742\n",
      "  (2, 25)\t0.35548195762874873\n",
      "  (3, 20)\t1.0\n",
      "  (4, 4)\t0.859534997767905\n",
      "  (4, 25)\t0.5110768901174535\n",
      "  (5, 25)\t1.0\n",
      "  (7, 17)\t0.9412322291499969\n",
      "  (7, 25)\t0.33776010837475723\n",
      "  (11, 21)\t0.5533799348242362\n",
      "  (11, 23)\t0.5366405664621315\n",
      "  (11, 4)\t0.5475363087512142\n",
      "  (11, 25)\t0.32556342048857684\n",
      "  (12, 4)\t1.0\n",
      "  (14, 25)\t1.0\n",
      "  (15, 37)\t0.7118629038754104\n",
      "  (15, 0)\t0.7023184506234106\n",
      "  (16, 11)\t0.6478335937139895\n",
      "  (16, 14)\t0.4007783298922675\n",
      "  (16, 23)\t0.6478335937139895\n",
      "  (17, 14)\t0.4986460786244059\n",
      "  :\t:\n",
      "  (727, 44)\t1.0\n",
      "  (728, 17)\t0.8124203314399383\n",
      "  (728, 25)\t0.5830722125629216\n",
      "  (729, 38)\t0.7482164865280394\n",
      "  (729, 30)\t0.4838684403668721\n",
      "  (729, 25)\t0.45392006091884396\n",
      "  (733, 46)\t0.8346729885507431\n",
      "  (733, 25)\t0.5507458598879985\n",
      "  (734, 33)\t0.7499328989220224\n",
      "  (734, 37)\t0.6615139054580876\n",
      "  (735, 35)\t0.7391763947283081\n",
      "  (735, 32)\t0.6735118836935696\n",
      "  (736, 43)\t1.0\n",
      "  (738, 18)\t1.0\n",
      "  (739, 47)\t0.6094124262000808\n",
      "  (739, 40)\t0.5963565650047202\n",
      "  (739, 19)\t0.5224704222907762\n",
      "  (740, 28)\t0.8668308595259199\n",
      "  (740, 25)\t0.4986023074290322\n",
      "  (741, 44)\t1.0\n",
      "  (742, 36)\t0.6520428640510826\n",
      "  (742, 48)\t0.6520428640510826\n",
      "  (742, 14)\t0.3868852631984358\n",
      "  (744, 3)\t1.0\n",
      "  (745, 31)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Testing the data with corpus below and comapring with the SkLearn's implementation:\n",
    "\n",
    "#importing corpus data\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "#SkLearn's code copied from the above cells\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features = 50)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "a, b, c = tf_idf(corpus, 50)\n",
    "print(('The shape of custom TFIDF vectorizer = {}'.format(tfidfvectorzr2(a,b).shape)),'\\n\\n','The shape of sklearn TFIDF Vectorizer ='.format(X.shape))\n",
    "print(c, '\\n',vectorizer.get_feature_names_out() )\n",
    "print('='*50, '\\nCustom TFIDF vector value')\n",
    "print(tfidfvectorzr2(a,b))\n",
    "print('\\n\\n Sklearn TFIDF vector value: ')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result for Task 2: \n",
    "\n",
    "We do not obtain the same vectors because, upon closer examination, it becomes evident that Scikit-learn's max_features selects features based on term frequency, whereas the features obtained through our custom implementation are ranked according to the IDF order. In the near future, we plan to write additional code to ensure that the custom feature selection process aligns with term frequency."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
